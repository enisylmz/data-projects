{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b22f9-d3fa-4584-abf6-5ad45b487180",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8aae96-f517-463c-a63f-2b0d07551373",
   "metadata": {},
   "source": [
    "### Basics of Statistics\n",
    "#### Descriptive Statistics\n",
    "- aims to describe an summarize a data set in a meaningful way\n",
    "- describe the collected data without drawing conclusions about a larger populuation\n",
    "##### Measures of Central Tendency\n",
    "- Mean: The avarage of the data = Sum of data / len of data\n",
    "- Median: The middle value of sorted data\n",
    "- Mode: The value that appear most frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd86285-4f43-44ba-b1d2-f5574cee741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  34.666666666666664\n",
      "Median:  37.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "data = [3,76,43,12,31,43]\n",
    "\n",
    "def mean(arr):\n",
    "    sum = 0\n",
    "    for i in arr:\n",
    "        sum += i\n",
    "    return sum/len(arr)\n",
    "\n",
    "def median(arr):\n",
    "    arr.sort()\n",
    "    length = len(arr)\n",
    "    if length % 2 == 1:\n",
    "        return arr[math.floor(length/2)]\n",
    "    else:\n",
    "        return (arr[int(length / 2)] + arr[int(length / 2 - 1)]) / 2\n",
    "\n",
    "print('Mean: ', mean(data))\n",
    "print('Median: ', median(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23794773-4891-43fb-af78-3482a126afb2",
   "metadata": {},
   "source": [
    "##### Measures of Dispersion \n",
    "- Describes how spread out the values in a data set are\n",
    "- Standart Deviation: Indicate the average distance beetween each data point and mean\n",
    "- Variance: The squared standart deviation = std_dev**2\n",
    "- Range: Difference between the max and min value\n",
    "- Interquartile Range: Represent the middle %50 of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc72941-35e4-4121-b355-0f4358dad2ab",
   "metadata": {},
   "source": [
    "### Inferential Statistics\n",
    "- Allows us to make a conclusion or inference about a population based on data from a sample\n",
    "#### Hypothesis\n",
    "- A statement we want to test\n",
    "#### Hypothesis Testing\n",
    "- A method for testing a claim about a parameter in a population using data measure in a sample "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d26d6c-e3fc-430f-a324-8f5ad3a49cfe",
   "metadata": {},
   "source": [
    "#### Level of Measurement\n",
    "- Which hypothesis test you can use depends on the level of measurement of your data\n",
    "- **Types**\n",
    "  - Nominal: Data can be categorized. Not possible to rank the categories, bar charts\n",
    "  - Ordinal: Data can be categorized and categories can be ranked (Satisfaction : unsatisfied , neutral, satisfied)\n",
    "  - Metric Variables(Interval and Ratio): Like ordinal but interval between values are equally spaced.(Income, Weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbbb85-361f-4f36-8119-857aeb354522",
   "metadata": {},
   "source": [
    "#### T-Test\n",
    "- A statistical test procudure\n",
    "- T-Test analyzes whether there is a significant difference between the means of two groups\n",
    "##### One sample t-test\n",
    "- We use it when we want to compare the mean of a sample with a know reference\n",
    "- **Null Hypothesis**: The sample mean is equal to the reference value\n",
    "- **Alternative Hypothesis**: The sample mean is not equal the reference value\n",
    "##### Independent samples t-test\n",
    "- We use it for independent samples when we want to compare the means of two independent groups or samples\n",
    "- **Null Hypothesis**: The mean values in both groups are the same \n",
    "- **Alternative Hypothesis**: The mean values in both groups are not the same\n",
    "##### Paired samples t-test\n",
    "- We use it to compare the means of two dependent groups\n",
    "- Example : How effective diet is Before diet and After Diet looking at the difference in weight on same sample\n",
    "- **Null Hypothesis**: The mean of the difference between the pairs is 0 \n",
    "- **Alternative Hypothesis**: The mean of the difference between the pairs is not zero\n",
    "##### Calculating T-Test\n",
    "- t-value = Difference between mean values / Standart deviation from the mean\n",
    "- if calculated T-value > Critical T-value = reject null hypothesis, accept otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9870d1-e0d1-48dd-9db9-6d52fe4b3101",
   "metadata": {},
   "source": [
    "#### Analysis of Variance (ANOVA)\n",
    "##### One-way ANOVA\n",
    "- Test whether there are statistically significant differences between three or more groups\n",
    "- **Null Hypothesis**: The mean values of the groups are equal\n",
    "- **Alternative Hypothesis**: The mean values of the groups are not equal, at least one mean is different\n",
    "- Level of measurement: Independent variable => Nominal such as drug type. Dependent variable => metric, blood pressure\n",
    "- Independence: the meassured vaslue of one group should not be influenced by the measured value of another group.\n",
    "- Normality: The data with in the groups should be normally distributed, less important as the sample size increases\n",
    "- Homogeneity: Variances in each group should be roughly the same\n",
    "- **F-Statistics(f_value) = Variance between groups / variance within groups**, also called Mean Squares in general.\n",
    "- **Sum_of_squares_between_the_groups(SS_btw) = sum(n_i(M_i - General average)\\**2)**, n_i: number of data in the ith group, M_i: mean of the ith group\n",
    "- **Sum_of_squares_within_the_groups(SS_wi) = sum(sum((x_i - M_i)\\**2))**, x_i: ith data in the group, M_i: mean of the ith group\n",
    "- **Degrees of freedom between the groups(df_btw) = number of groups - 1**\n",
    "- **Degrees of freedom within the groups(df_wi) = number of values - number of groups**\n",
    "- **Variance between groups = SS_btw / df_btw**\n",
    "- **Variance within groups = SS_wi / df_wi**\n",
    "- **If p_value lower than significance level, 0.05 in general, we get a significant difference and we reject the Null Hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41ab4b-d177-4bf4-9419-d26e85a2d396",
   "metadata": {},
   "source": [
    "##### One-way ANOVA\n",
    "- Test whether there are statistically significant differences between three or more groups\n",
    "- **Null Hypothesis**: The mean values of the groups are equal\n",
    "- **Alternative Hypothesis**: The mean values of the groups are not equal, at least one mean is different\n",
    "- Level of measurement: Independent variable => Nominal such as drug type. Dependent variable => metric, blood pressure\n",
    "- Independence: the meassured value of one group should not be influenced by the measured value of another group.\n",
    "- Normality: The data with in the groups should be normally distributed, less important as the sample size increases\n",
    "- Homogeneity: Variances in each group should be roughly the same\n",
    "- **F-Statistics(f_value) = Variance between groups / variance within groups**, also called Mean Squares in general.\n",
    "- **Sum_of_squares_between_the_groups(SS_btw) = sum(n_i(M_i - General average)\\**2)**, n_i: number of data in the ith group, M_i: mean of the ith group\n",
    "- **Sum_of_squares_within_the_groups(SS_wi) = sum(sum((x_i - M_i)\\**2))**, x_i: ith data in the group, M_i: mean of the ith group\n",
    "- **Degrees of freedom between the groups(df_btw) = number of groups - 1**\n",
    "- **Degrees of freedom within the groups(df_wi) = number of values - number of groups**\n",
    "- **Variance between groups = SS_btw / df_btw**\n",
    "- **Variance within groups = SS_wi / df_wi**\n",
    "- **If p_value lower than significance level, 0.05 in general, we get a significant difference and we reject the Null Hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c58834-686b-46c8-b079-4bf4bac9cfa0",
   "metadata": {},
   "source": [
    "##### Two-way ANOVA\n",
    "- Extension of One-way ANOVA. The difference is two-way ANOVA has two independent variable. The independent variables are called factors.\n",
    "- Factor is a nominal variable\n",
    "- We can answer three things:\n",
    "  - Does the factor 1 have an effect on the dependent variable?\n",
    "  - Does the factor 2 have an effect on the dependent variable?\n",
    "  - Is there an interaction between two factors?\n",
    "- **Null Hypothesis:**\n",
    "  - There is no significant difference between the groups of the first factor\n",
    "  - There is no significant difference between the groups of the second factor\n",
    "  - One factor has no effect on the effect of the other factor\n",
    "- **Alternative Hypothesis:**\n",
    "  - There is a significant difference between the groups of the first factor\n",
    "  - There is a significant difference between the groups of the second factor\n",
    "  - One factor has an influence on the effect of the other factor\n",
    "- Level of measurement: Two independent variables => Nominals such as drug types drug A and drug B as first variable and the gender as second variable. Dependent variable => metric, blood pressure\n",
    "- Independence: the meassured value of one group should not be influenced by the measured value of another group.\n",
    "- Normality: The data with in the groups should be normally distributed, less important as the sample size increases\n",
    "- Homogeneity: Variances in each group should be roughly the same\n",
    "- **Sum_of_squares_total(SS_tot) = Sum_of_squares_A(SS_A) + Sum_of_squares_B(SS_B) + Sum_of_squares_AB(SS_AB) + Sum_of_squares_error(SS_err)**\n",
    "- **F-Statistics(f_value) = Variance between groups / variance within groups**, also called Mean Squares in general.\n",
    "- **Sum_of_squares_total(SS_tot) = sum((x_mij - General mean)\\**2)**, x_mij: every data value\n",
    "- **Sum_of_squares_between_the_groups(SS_btw) = sum(sum((AB_ij - general mean)\\**2))**, AB_ij: Mean value of each group, M_i: mean of the ith group\n",
    "- **Sum_of_squares_A(SS_A) = n * q * sum((A_i - General mean)\\**2)**, A_i: Mean values of the factor A, n: Number of people per group, q: Number of groups Factor B\n",
    "- **Sum_of_squares_B(SS_B) = n * q * sum((B_i - General mean)\\**2)**, B_i: Mean values of the factor B, n: Number of people per group, p: Number of groups Factor A\n",
    "- **Sum_of_squares_total(SS_AB) = Sum_of_squares_between_the_groups(SS_btw) - Sum_of_squares_A(SS_A) - Sum_of_squares_B(SS_B)**\n",
    "- **Sum_of_squares_error(SS_err) = sum(sum(sum((x_mij - AB_ij)\\**2)))**, x_mij: Individual values, AB_ij: mean values of the groups\n",
    "- **Degrees of freedom total(df_tot) = n * p * q - 1**, n: Number of people per group, p: Number of groups Factor A, q: Number of groups Factor B \n",
    "- **Degrees of freedom between the groups(df_btw) = p * q - 1**, p: Number of groups Factor A, q: Number of groups Factor B\n",
    "- **Degrees of freedom A(df_A) = p - 1**, p: Number of groups Factor A\n",
    "- **Degrees of freedom B(df_B) = p - 1**, q: Number of groups Factor B\n",
    "- **Degrees of freedom AB(df_AB) = (q - 1)(p - 1)**, p: Number of groups Factor A, q: Number of groups Factor B\n",
    "- **Degrees of freedom error(df_err) = (n - 1) * p * q**, n: Number of people per group, p: Number of groups Factor A, q: Number of groups Factor A\n",
    "- **Variance total groups = SS_total / df_total**\n",
    "- **Variance between groups = SS_btw / df_btw**\n",
    "- **Variance A = SS_A / df_A**\n",
    "- **Variance B = SS_B / df_B**\n",
    "- **Variance error = SS_err / df_err**\n",
    "- **If p_value lower than significance level, 0.05 in general, we get a significant difference and we reject the Null Hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913e93c-66ea-4ee2-a514-dd0f319a47d2",
   "metadata": {},
   "source": [
    "##### Repeated measures ANOVA\n",
    "- Tests whether there is a statistically significant difference between three or more dependent samples\n",
    "- In a dependent sample the same test unit are measured several times under different conditions\n",
    "- It is similar to the Paired T-Test but it tests three or more groups\n",
    "- **Null Hypothesis**: There are no differences between the dependent groups\n",
    "- **Alternative Hypothesis**: There is a difference between the dependent groups\n",
    "- Normality: The data with in the groups should be approximately normally distributed, less important as the sample size increases\n",
    "- Sphericity: The variance of the differences between all combinations of factor levels(ex. time points) should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73413b6c-1415-495a-bb49-fbe61563eff6",
   "metadata": {},
   "source": [
    "##### Calculations of Repeated ANOVA\n",
    "<img src='repeated_anova_calculations.png' width=\"1300\">\n",
    "\n",
    "- QS_err = QS_in - QS_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ef0bd-42b7-44d5-adc1-aa0c42bd6335",
   "metadata": {},
   "source": [
    "#### Mixed Model ANOVA (TRwo-way ANOVA with repeated measures)\n",
    "- A statistical method used to analyze data that involve between-subjcts-factors and within-subjects-factors.\n",
    "\n",
    "<img src='mixed_model_anova_visualization.png' width=\"1000\">\n",
    "\n",
    "- We can answer three things:\n",
    "  - Does the within-subject factor have an effect?\n",
    "  - Does the between-subject factor have an effect?\n",
    "  - Is there an interaction between the two factors?\n",
    "- **Null Hypothesis:**\n",
    "  - Within-Subject: The mean values of the different measurement time points do not differ.\n",
    "  - Between-Subject: The mean values of the different groups of the between-subject factor do not differ.\n",
    "  - Interaction: One factor has no influence on the effect of the other factor.\n",
    "- **Alternative Hypothesis:**\n",
    "  - Within-Subject: The mean values of the different measurement time points differ.\n",
    "  - Between-Subject: The mean values of the different groups of the between-subject factor differ.\n",
    "  - One factor has an influence on the effect of the other factor\n",
    "- Normality: The data with in the groups should be approximately normally distributed, less important as the sample size increases\n",
    "- Homogeneity: Variances in each group should be the same, needs to be true for both within subjects and between subject factors\n",
    "- Homogeneity of Covariances (Sphericity): The variance of the differences between all combinations of factor levels(ex. time points) should be the same. This applies to the within subjects factor.\n",
    "- Independence of Observations: The observations are independent of each other.\n",
    "- No Significant Outliers: Outliers can have a disproportionate effect on ANOVA, potentially leading to misleading results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8048e4-be06-4c9e-9aca-1cc6552fb7db",
   "metadata": {},
   "source": [
    "**Note: So far we explored parametric tests. They require assumptions, like normality. What if these assumptions are not met by the data. This is where non-parametric test come into play.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc55d82-b801-487a-81ab-ed90047fb138",
   "metadata": {},
   "source": [
    "#### Non-parametric Tests\n",
    "- If the data not normally distributed non-parametric tests are used.\n",
    "- In general non-parametric tests makes fewer assumptions than parametric tests.\n",
    "- Parametric tests are generally more powerful than non-parametric tests.\n",
    "- Raw data is used for parametric tests, whereas ranks of the raw data used in non-parametric tests.\n",
    "\n",
    "<img src='parametric_and_non_parametric_tests_classification.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256835b8-23a1-4511-b606-b05d2a79e664",
   "metadata": {},
   "source": [
    "#### Tests for Normal Distribution\n",
    "##### Check normal distribution analytically\n",
    "- Kolmogorov-Smirnov Test, Shapiro-Wilk Test, Anderson-Darling Test\n",
    "- Null Hypothesis: The data are normally distributed.\n",
    "- If p_value is smaller than the significance level, in general 0.05, normal distribution is **not** assumed, normally distributed otherwise.\n",
    "graphical_test_of_normal_distribution\n",
    "<img src='graphical_test_of_normal_distribution.png' width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d265d-5a3d-47fd-935c-cadc884285ec",
   "metadata": {},
   "source": [
    "#### Levene's test for equality of variances\n",
    "- **Null Hypothesis:** The variances of the groups are equal.\n",
    "- **Alternative Hypothesis:** At least one of the groups has a different variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b9b04-7dbf-4cbd-ab5b-a199da31fb63",
   "metadata": {},
   "source": [
    "#### Mann-Whitney Test (non-parametric for Independent samples T-Test)\n",
    "- Tests whether there is a difference between two independent samples.\n",
    "- Checks whetere there is a rank sum difference.\n",
    "- We sort all people from the smallest to the largest value\n",
    "- The advantage of taking the rank sums rather than the difference in means is that the data need not to be normally distributed.\n",
    "- **Null Hypothesis:** In the two samples, the rank sums do not differ significantly.\n",
    "- **Alternative Hypothesis:** In the two samples, the rank sums do differ significantly.\n",
    "\n",
    "- **Calculations:**\n",
    "<img src='mann_whitney_test_calculations.png' width=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438811c-d67c-4320-ac36-c4d41ea4fa4e",
   "metadata": {},
   "source": [
    "#### Wilcoxon-Test(Non-paramateric for Paired T-test)\n",
    "- Tests whether there is a difference between two dependents samples.\n",
    "- The data do not have to be normally distributed.\n",
    "- Only two dependent random samples with at least ordinally scaled characteristics need to be available.\n",
    "- **Null Hpothesis:** In the population, the central tendencies of the two dependent samples are the same.\n",
    "- **Alternative Hpothesis:** In the population, the central tendencies of the two dependent samples are unequal.\n",
    "<img src='wilcoxon_test_example.png' width=\"800\">\n",
    "\n",
    "<img src='wilcoxon_test_example_calculations.png' width=\"800\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77459c24-815a-457b-bc04-0ed8106c7a36",
   "metadata": {},
   "source": [
    "##### Kruskal-Wallis-Test(Non-Parametric for ANOVA)\n",
    "- Checks wheter there is a difference between several independent groups\n",
    "- Non-parametric counterpart to single factor analysis of variance\n",
    "- Is there a difference in the rank totals?\n",
    "- How must the variables be scaled?\n",
    "  - A nominal or ordinal variable with more than two expressions\n",
    "  - A metric or ordinal variable\n",
    "- Assumptions:\n",
    "  - Only several independent random samples with at least ordinally scaled characteristics must be available\n",
    "  - The variables do not have to satisfy a distribution curve.\n",
    "- **Null Hypothesis:** The independent samples all have the same central tendency and therefore come from the same population. In other words there is no difference in the rank sums.\n",
    "- **Alternative Hypothesis:** At least one of the independent samples does not have the same central tendency as the other samples and therefore come from at different population.\n",
    "\n",
    "  <img src='kruskal_wallis_test_example.png' width=\"650\">  <img src='kruskal_wallis_test_example_2.png' width=\"650\"><img src='kruskal_wallis_test_example_3.png' width=\"650\">\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ceb186a-8e71-472f-b1c5-c37f6efff37c",
   "metadata": {},
   "source": [
    "# Friedman-Test(Deleted by accident)\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c6ac25-6021-485d-8296-11172212c037",
   "metadata": {},
   "source": [
    "# Friedman-Test(Deleted by accident)\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80943e04-7e8d-4b84-a493-3a826dd0c3ee",
   "metadata": {},
   "source": [
    "# Friedman-Test(Deleted by accident)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e1523-ac4e-4dfa-b220-e945af417fba",
   "metadata": {},
   "source": [
    "### Chi-Square-Test\n",
    "- Its a powerful tool for analyzing nominal data\n",
    "- It is used when we want to determine if there is relationship between, two categorical variables(ex. Gender).\n",
    "- The assumption for the test is that **expected frequencies** per cell are greater than 5.\n",
    "- It uses only the categories and not rankings.\n",
    "- **Null Hpothesis:** There is no relationship between categories\n",
    "- **Alternative Hpothesis:** There is relationship between categories.\n",
    "\n",
    "<img src='chi_square_test_example_case.png' width=\"550\">\n",
    "\n",
    "<img src='chi_square_test_example_calculations.png' width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc30c7-ce67-4d1d-bbc2-85e1cffaf800",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "- It is a statistical method used to measure the relationship between two variables.\n",
    "- In correlation analysis, we usually want to know 2 things:\n",
    "  - How strong the correlation is\n",
    "  - In which direction the correlation goes.\n",
    "\n",
    "<img src='correlation_strength.png' width=\"650\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b8f596-4fe0-4624-9f06-3d2a2c2b07cb",
   "metadata": {},
   "source": [
    "### Pearson Correlation(r)\n",
    "- A statistical measure that quantifies the relationship between two variables.\n",
    "- The linear relationship of metric variables is measured.\n",
    "  \n",
    "<img src='pearson_correlation_formula.png' width=\"675\">\n",
    "\n",
    "- **Null Hpothesis:** The correlation coefficent does not differ significantly from zero. There is no linear relationship\n",
    "- **Alternative Hpothesis:** The correlation coefficent differs significantly from zero. There is a linear relationship\n",
    "\n",
    "<img src='pearson_correlation_t_test.png' width=\"475\">\n",
    "\n",
    "- A p-value can then be calculated from the test statistic t. If p value < significance level, then the null hypothesis is rejected otherwise it is not.\n",
    "- We must distinguşsh wheter we just want to calculate the Pearson correlation coefficient or wheter we want to test a hypothesis.\n",
    "- If there is a non linear relationship we cannot tell from the Pearson correlation coefficient.\n",
    "- If we want to test a hypothesis the two variables must also be normally distributed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb4a857b-bdaa-40e4-86b8-397eb7513280",
   "metadata": {},
   "source": [
    "### Spearman Correlation(r)\n",
    "- The spearman rank correlation is the non parametric counterpart of the Pearson Correlation\n",
    "- Spearman Correlation use the ranks of the data.\n",
    "- After assigning the ranks we use Pearson Correlation anlaysis for calculations\n",
    "  \n",
    "<img src='spearman_correlation_coefficient.png' width=\"675\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21890a-22f6-43ee-82bd-e871bf06eafc",
   "metadata": {},
   "source": [
    "### Kendall's Tau\n",
    "- In contrast to Pearson Correlation, Kendall's rank correlation is a non-parametric test procedure.\n",
    "- The two variables need only have ordinal scale levels.\n",
    "- Kendall's tau should be preffered over Spearman's correlation, if very few data with many rank ties are available.\n",
    "  \n",
    "<img src='kendalls_tau_formula.png' width=\"375\">\n",
    "\n",
    "- **Concordant**: A pair of observations is concordant if the direction of their relationship between the predictor(s) and the outcome is the same (i.e., both increase or both decrease together).\n",
    "- **Discordant**: A pair of observations is discordant if the direction of their relationship between the predictor(s) and the outcome is opposite (i.e., one increases while the other decreases).\n",
    "\n",
    "- **Null Hpothesis:** The correlation coefficent does not differ significantly from zero. There is no relationship\n",
    "- **Alternative Hpothesis:** The correlation coefficent differs significantly from zero. There is a relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20ce86-2505-4103-a1bd-2e0712faecf5",
   "metadata": {},
   "source": [
    "### Point biserial correlation\n",
    "- It is a special case of Pearson Correlation and examines the relationship between a dichotomous variable and metric variable.\n",
    "- **Dichotomous variable:** It is a variable with two values. Ex. gender: male and female, smoking status: smoker, non-smoker.\n",
    "\n",
    "<img src='point_biserial_formula.png' width=\"375\"> \n",
    "\n",
    "- x1: mean of the people who has value 0 in dichotomous variable\n",
    "- x2: mean of the people who has value 1 in dichotomous variable\n",
    "- n1: total number of the people who has value 0 in dichotomous variable\n",
    "- n2: total number of the people who has value 1 in dichotomous variable\n",
    "- n: total number of the people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcd395-bdfa-441e-8dbe-c902215bf0cc",
   "metadata": {},
   "source": [
    "### Causality\n",
    "- It is a relationship between a cause and an effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91383be4-1157-42e3-a1d6-e6cf397b3b04",
   "metadata": {},
   "source": [
    "## Regression Analysis\n",
    "- A regression analysis is a method for modeling relationships between variables it makes it possible to infer or predict a variable based on one or more other variables.\n",
    "- The variable we want to infer or predict is called **the dependent variable**(input variable).\n",
    "- The variables we uses for predictions are called **the independent variables**(outcome, target variable).\n",
    "- Measure the influence of one or more variables on another variable.\n",
    "- Prediction of one variable by one or more other variables.\n",
    "\n",
    "<img src='regression_types.png' width=\"575\"> <img src='regression_examples.png' width=\"575\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073ec51-db0f-4235-9526-1f122346bd84",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "<img src='simple_linear_regression_formula1.png' width=\"275\"> **s_y**: standart deviation of dependent variable, **s_x**: standart deviation of independent variable\n",
    "\n",
    "- **a = mean of the dependent variable - b * mean of the independent variable**\n",
    "- Assumptions:\n",
    "  - Linear relationship: If the relationship is non-linear the straight line cannot fulfill this requirement.\n",
    "  - Independence of erros: the error of one point doesn't affect another.\n",
    "  - Homoscedasticity(Equal variance of errors):\n",
    "  <img src='linear_regression_homoscedasticity.png' width=\"375\">\n",
    "\n",
    "  - The errors should be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02d002-e9fc-481f-bc7e-eff3132ccf31",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "<img src='multiple_linear_regression_formula.png' width=\"375\">\n",
    "\n",
    "- All 4 assumptions of simple linear regression is valid. There is one more assumption in multiple linear regression:\n",
    "  - No multicollinearity:  Multicollinearity means the effect of individual variables cannot be clearly seperated. It is important when we assign the influnce coefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2044813-b543-4838-b7e3-6718854e4f08",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- In a logistic regression the dependent variable is a binary variable.\n",
    "- The goal of logistic regression is to estimate the probability of occurence\n",
    "\n",
    "<img src='logistic_regression_formula.png' width=\"375\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff76476-92b8-4580-821c-90b550cef1e7",
   "metadata": {},
   "source": [
    "### k-means clustering\n",
    "- It is a powerful method used to identify hidden groups or clusters within our data\n",
    "- The k-means method clusters your data points on a given number of clusters.\n",
    "- The number of clusters is the 'k' in the k-means.\n",
    "\n",
    "<img src='k_means_cluster_analysis.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281bbfc-d478-4622-903e-2debadadeb80",
   "metadata": {},
   "source": [
    "### Confidence Interval(CI)\n",
    "\n",
    "<img src='confidence_interval_formula.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913eae38-d7c5-47bd-a411-7de033f1ccb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
